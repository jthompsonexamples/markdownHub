---
title: "Spatial Analysis"
subtitle: "Using Different Data Types to Solve Problems"
author: "MAJ Justin Thompson"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      highlight = TRUE,
                      root.dir = "D:/Fort Lee/ProgrammingNew/Lesson10")
require(tidyverse)
require(lubridate)
require(sf)
require(leaflet)
require(magrittr)
require(RColorBrewer)
```

## Problem

I have a bunch of locations and I need information about what's there. Specifically, the sponsor wants to know the population density of the surrounding area as well as the distance between the locations to inform business expansion decisions.

### Data sources
Authoritative data is key. Data with a strong pedigree (even if it isn't the most accurate) gives credibility to study results. In this case, the US Census Bureau is the place to go. They have authoritative population data as well as shape files defining the area (down to a fairly small area) in which those populations dwell.

We'll use the [US Census Data Tables](https://data.census.gov/cedsci/) and the associated [shape files](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2010.html) to solve this problem by getting data down to the census tract level.  *Note: The census webpage is kind of a pain to use.*

### Load and manipulate the data

I've gotten the external data, now get it into R. I'll use the `sf` package in this example, but there are other packages and methods out there (e.g. `sp`, `rgdal`, `geosphere`).

I'll do the easy one first; the location data. The location file is tab-delimited so use `read_delim` instead of `read_csv`. Don't forget to set the delimiter.

```{r, message=F}
mnLoc <- read_delim('./Data/mnLocations.txt', delim='\t')
```

```{r, message=F, echo=F, warning=F}
knitr::kable(head(mnLoc))
```
The data has the coordinates, the nearest city, and the nearest airfield. Look at the longitude information. The standard convention is negative longitude values for locations in the western hemisphere (same for latitude in the southern hemisphere). So make the conversion. 
```{r}
mnLoc %<>% mutate(LON = -LON) 
```

Now read in the census data and shape files. I stored the shape files inside a folder called `mnCensusShape`. That folder name is the first argument to the `st_read` function. The layer is the file name of the `.shp` file inside that directory.

```{r}
mnShape <- st_read('./Data/mnCensusShape', layer='tl_2010_27_tract10')
mnPop <- read_csv('./Data/mnPopTable/mnPop.csv')
```
R gives us a message with helpful information about the shape file, which is handy. Now we know that its a set of 1338 polygons, it has a Coordinate Reference System (CRS) assigned, and 12 data fields. Looking at the `Data` window in RStudio the `mnShape` object looks just like a data frame, and it kind of is. Its a data frame with sprinkles on top.
```{r}
class(mnShape)
names(mnShape)
```
The 12 data fields from the earlier message mean there are 12 columns of data in the data frame. The 13th column has the spatial information. Notice there isn't any population data, only some cryptic codes. 
```{r, echo=F}
knitr::kable(mnShape %>% select(STATEFP10:NAME10) %>% slice(1:5) %>% st_drop_geometry())
```

These codes map to location names in a very large data dictionary, but its easier to use codes when doing database operations (like joins) and computers like integers better than strings. Now take a look at the population file.
```{r, echo=F}
knitr::kable(mnPop %>% slice(1:5))
```
This one has a `GEOID` variable as well (and a nonsense first row), but values look different than those in the shape file's data. 
```{r, echo=F}
knitr::kable(
  bind_cols(
  mnPop %>% slice(2:6) %>% select(GEO_ID),
  mnShape %>% slice(1:5) %>% select(GEOID10) %>% st_drop_geometry()
))
```
Turns out they are the same, the population file just has additional information. But everything after "US" maps to a value in `GEOID10`, so strip everything except what's after "US". Also drop the nonsense first row and convert the `POP` column to numeric. 
```{r}
mnPop %<>%
  slice(-1) %>% 
  rename('POP' = 'P001001') %>% 
  mutate(newGEOID = str_split(GEO_ID, 'US') %>% map_chr(.,2),
         POP=as.numeric(POP))
```

### Create spatial objects

Now the data are ready I'll make my spatial objects. First, join the population data frame with the census tract spatial data frame. I'll use an inner join and then check if there are any missing values. 
```{r}
popShape <- 
  inner_join(
    mnShape,
    mnPop,
    by=c('GEOID10' = 'newGEOID')
  )
nrow(popShape) == nrow(mnShape)
```
Everything made it. Now we've got a population map.
```{r, echo=F, message=F, warning=F}
bins <- c(0, 100, 500, 1000, 2000, 3000, 4000, 5000, 8000, Inf)
pal <- colorBin("YlOrRd", domain = popShape$POP, bins = bins)
popShape %>% 
  leaflet() %>% 
  addPolygons(
    stroke = T,
    label = ~paste('Pop: ', POP),
    color = 'black',
    weight = 1,
    fillColor = ~pal(POP),
    fillOpacity = 0.7
  ) %>% 
  addLegend(pal = pal, values = ~POP, opacity = 0.7, title = NULL,
            position = "bottomright")
```
Next, create a spatial object using the locations. The `st_as_sf` function creates a spatial object from a data frame, just tell it which columns have the coordinates.
```{r}
locShape <- 
  mnLoc %>% 
  st_as_sf(
    coords=c('LON', 'LAT'),
    dim='XY'
  )
class(locShape)
```

The new object does not have any CRS information, so analysis is pretty much impossible. Since both `locShape` and `popShape` use lat/lon add the CRS info from `popShape` so the two object will line up.
```{r}
st_crs(locShape) <- st_crs(popShape)
```
Did it work?
```{r, echo=F, message=F, warning=F}
leaflet(popShape) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addPolygons(
    weight = 1,
    color = 'black',
    fillColor = 'white',
    fillOpacity = 0.2
  ) %>% 
  addCircleMarkers(
    data = locShape,
    radius = 5,
    color = 'red',
    stroke=F,
    fillOpacity = 1,
    label = ~nearestCity
  )
```

### Analyze
What do I need to know to calculate population density? Pretty simple, population and area ($\frac{pop}{area}$). I already have population thanks to the previous join, now I need the area of each census tract. Thankfully, I don't need to look around online to find a data set and do another join. I can use the shape file and a function called `st_area` to get the area of each polygon in my shape file. Pay attention to units, the CRS for `mnShape` has units `metre` (the Queen's spelling) so the area is in $m^2$. Convert to $km^2$.

```{r}
popShape %<>%
  mutate(area = st_area(popShape)/(1000**2))
# change the display unit attribute
attr(popShape$area, 'units')$numerator <- c('km', 'km') 
```
Now apply the formula $\frac{pop}{area}$ and get the population density for each census tract in $\frac{pop}{km^2}$.
```{r}
popShape %<>%
  mutate(density = POP/area)
```
I've got population densities for each census tract, next I have to get the density for each location. I could use the maps I've made and do a manual look-up, but that's not scale-able. If the sponsor wants me to do this again for many locations I'm in a bind and stuck doing mind-numbing nug work.

Instead, leverage spatial analysis tools. I can ask R to find which polygons inside `popShape` a location in `locShape` resides and use that information to select the appropriate population density (or any other information contained in the `popShape` data frame). Use a function `st_within` to create a list of indices indicating which polygon a specific location resides.
```{r}
intersectList <- st_within(locShape, popShape)
sum(sapply(intersectList, length)==0) # check if any failed
intersectIndex <- unlist(intersectList) # convert list to a vector
```

`st_within` generates a list of indices where the first item in the list corresponds to the first spatial object in the first argument, `locShape` in our case. I'll use those indices to add a column to `locShape` with the population density.
```{r}
locShape$density <- popShape$density[intersectIndex]
```

```{r, echo=F}
knitr::kable(locShape %>% st_drop_geometry())
```

Done. Now I have a scale-able, reusable script to get the population densities or any other demographic data I decide to pull out of the US Census Bureau's database. As long as I can map said data to a census tract GEOID that is. But the sponsor probably wants a nice report with pretty pictures.

### Visualize
```{r, message=F, warning=F}
locShape %<>% 
  mutate(
    lab =
      paste(
        nearestCity,
        paste('Density:', round(density, 2), '/ km^2'),
        sep = '<br/>'))

bins <- c(0, 1000, 2000, 3000, 4000, 5000, 8000, Inf)
pal <- colorBin("YlOrRd", domain = popShape$POP, bins = bins)
popShape %>% 
  leaflet() %>% 
  addPolygons(
    stroke = T,
    label = ~paste('Pop: ', POP),
    color = 'black',
    weight = 1,
    fillColor = ~pal(POP),
    fillOpacity = 0.7
  ) %>% 
  addCircleMarkers(
    data = locShape,
    radius = 3,
    color = 'black',
    stroke=F,
    fillOpacity = 1,
    label = lapply(locShape$lab, htmltools::HTML),
    labelOptions = labelOptions(noHide = T, textsize = "8px")
  ) %>% 
  # addPopups(
  #   data = locShape,
  #   popup = lapply(locShape$lab, htmltools::HTML)
  # ) %>% 
  addLegend(pal = pal, values = ~POP, opacity = 0.7, title = NULL,
            position = "bottomright")
```

### Distance
All that intersecting the data manipulation is a pain. I saved the easy part of the analysis for last, getting the distance between points. As long as I have a spatial points object with a CRS applied its a single function call to get a dense distance matrix. Again, pay attention to the units. The function takes units from the CRS.
```{r}
st_crs(mnShape)
```
Again, metres (still the Queen's spelling). So our distance matrix is in meters (correct spelling, thanks Noah Webster). The `st_distance` function calculates the distance between spatial objects. If we give it a single object full of points, it will return a dense distance matrix with each pairwise distance, which may take a while to calculate depending on the size. For display purposes I'll subset the data.
```{r}
distMat <- st_distance(locShape %>% slice(1:5))
```

```{r, echo=FALSE}
distMat <- as.data.frame(distMat)
names(distMat) <- locShape$nearestCity[1:5]
rownames(distMat) <- locShape$nearestCity[1:5]
knitr::kable(distMat)
```

If the data is lat/lon (as this is) the default method is Great Circle Distance since the Earth is sphere-ish. Other methods are available (Euclidean for example). This is straight-line distance, not driving distance. If you want that, get a Google API key and get ready to pay some money.

`st_distance` does not work for lines or polygons since they are just a collection of points, so which point to use? The `geosphere` package has additional tools to answer questions like "What's the closest point on a line from a particular point?"

### Excursion
ORSAs generally do excursions based insights discovered during the assigned study or areas we feel the sponsor may want to know but didn't ask. Essentially its a way to show how awesome we are to generate more work (and more money). 

Most businesses have defined service areas. For instance, McDonald's does extensive market analysis to ensure locations aren't too close and interfere with each other. So the mean population density of a service area may interest the sponsor. We'll assume a simple, circular service area to start and get more information from the sponsor later to better inform those areas.

To reliably create circular polygons with a given radius, convert the locations from lat/long CRS to UTM (built around meters). To do this I need to know the UTM zone for each point, so I'll use the same intersection method as above only now using a shape file with global UTM zones.
```{r}
utm <- st_read('./Data/World_UTM_Grid', layer='0f893164-d038-48ff-98dd-9fefb26127d3202034-1-145zfwr.nwf1')

mnLocNew <- 
  mnLoc %>% 
  st_as_sf(
    coords=c('LON', 'LAT'),
    dim='XY'
  )

st_crs(mnLocNew) <- st_crs(utm)  
intersectList <- st_intersects(mnLocNew, utm)
sum(sapply(intersectList, length)==0)
intersectIndex <- unlist(intersectList)

mnLocNew %<>%
  mutate(utmZone = utm$ZONE[intersectIndex])
```
Now I've got UTM zones for each point. Next, create a circle of a given radius around each point and convert that set of circles into a spatial object. There is no straightforward function to accomplish this task so I wrote my own.

```{r, warning=F}
# function to create a circle from a spatial point
circleDraw <- function(vertex, r, n=1000){
  theta <- seq(0, 2*pi, length.out=n)
  x <- (cos(theta)*r)+vertex[1]
  y <- (sin(theta)*r)+vertex[2]
  return(data.frame(x=x, y=y))
}
# function to create the set of circles and attach that
  # geometry to the location data frame
createServiceArea <- function(pts, rad, parentData, crs){
  for(i in 1:nrow(pts)){
  # transform using the zone information
    ptsTrans <-
      st_transform(
        pts[i,],
        paste0(
          '+proj=utm +zone=',
          pts$utmZone[i],
          ' +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0'
          )
        )
  
    df <- circleDraw(st_coordinates(ptsTrans), rad) # draw the circle
    cir1 <- st_polygon(list(as.matrix(df))) # turn that into a polygon
    cir1 <- st_sfc(cir1, crs=st_crs(ptsTrans)) # turn that into a simple feature
    # transform to latlong
    cir1Trans <-
      st_transform(
        cir1,
        crs=crs
      )
    # put them into a data structure
    if(i==1){
      cirList <- st_sf(mnLoc[i,], geom = cir1Trans)
    }else{
      cirList <- bind_rows(cirList, st_sf(mnLoc[i,], geom = cir1Trans))
    }
    # cirList[[i]] <- cir1Trans
  }
  return(cirList)
}
serviceCircles <- 
  createServiceArea(mnLocNew, 
                    rad = 20000, 
                    parentData = mnLoc,
                    crs=st_crs(popShape))

```

With these circles I can estimate the population inside of the circular service area (20 km radius in this example). Again, I'll use the intersect method to find the census tracts which intersect the service area and sum the total population.

```{r}
intersectList <- st_intersects(serviceCircles, popShape)
sum(sapply(intersectList, length)==0)
intersectList[[1]]
```

```{r}
# quick example plot
popShape[intersectList[[1]],] %>% 
  st_geometry() %>%
  plot(axes=T)
serviceCircles[1,] %>% 
  st_geometry() %>% 
  plot(add=T, col='red')
```

Each item in `intersectList` contains a vector of all the indices of the census tracts which intersect a particular service area. Use that to select the relevant portions of the POP column from `popShape` and sum.

```{r}
locShape %<>% 
  mutate(
    servicePop = 
      sapply(
        intersectList, 
        function(i) 
          sum(popShape %>% 
                slice(i) %>% 
                pull(POP))
        )
    )
```

```{r, echo=F}
knitr::kable(st_drop_geometry(locShape) %>% select(-lab))
```

Final visualization bringing it all together.

```{r, message=F, warning=F}
locShape %<>% 
  mutate(
    lab =
      paste(
        nearestCity,
        paste('Density:', round(density, 2), '/ km^2'),
        paste('Serviced Pop:', servicePop),
        sep = '<br/>'))

bins <- c(0, 1000, 2000, 3000, 4000, 5000, 8000, Inf)
pal <- colorBin("YlOrRd", domain = popShape$POP, bins = bins)
popShape %>% 
  leaflet() %>% 
  addPolygons(
    stroke = T,
    label = ~paste('Pop: ', POP),
    color = 'black',
    weight = 1,
    fillColor = ~pal(POP),
    fillOpacity = 0.7
  ) %>% 
  addPolygons(
    data = serviceCircles,
    stroke = FALSE,
    color = 'black',
    fillColor = 'blue',
    fillOpacity = 0.4
  ) %>%
  addCircleMarkers(
    data = locShape,
    radius = 4,
    color = 'black',
    stroke = F,
    fillOpacity = 1,
    label = lapply(locShape$lab, htmltools::HTML)
  ) %>% 
  addLegend(pal = pal, values = ~POP, opacity = 0.7, title = NULL,
            position = "bottomright")
```
