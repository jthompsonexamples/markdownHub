---
title: "KNN-Introduction to supervised models"
author: "Q Course"
output:
  html_document:
    df_print: paged
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      root.dir = 'C:\\Users\\bexar_000\\Desktop\\School\\WorkFromHome\\New Study Stuff\\Q Course\\classFolders\\Advanced - Day 5')
library(class)
require(tidyverse)
require(magrittr)
```

## Introduction

Machine Learning is merely a tool that assists people in making use of the world's data.  Fundamentally, machine learning involves building mathematical models to help understand data (Python Data Science Handbook).
Machine learning algorithms are divided into categories according to their purpose. Understanding the purpose of the model, determines which algorithm one might select. Predictive models: The learning algorithm attempts to to discover and model the relationship between the response or target and the input data.  Predict does not always mean temporal as it could be a classification.  The process of training a predictive model is **supervised learning**.  Supervised Learning means the response variable provides a means for the analyst to determine how the model is performing.  These models can further be categorized by **classification models** or **numerical predictors**. Classification models determine if an email is spam or a person will purchase cheese spread. Numerical predictors use some sort of regression to determine an amount such as the number of meals ready to eat the Army will need in 2021.

**Descriptive models** summarize data in new ways.  If we can classify U.S. Army recruits based on their interests we can target those areas for recruiting.  Notice we are not predicting anything.  We just say, people that play Fortnight tend to be young men who fall into group A.  Young men who join the Army tend to be in group A too.  Let's put Army posters in Fortnight games.  Here there is no real target of output to measure our performance of the model.  This is called **unsupervised learning**.  Some examples of these models are K-means clustering and association rules.

In machine learning we generally divide data into three groups. This enables us to make better models.

**Training Dataset**: The sample of data used to fit the model.
The actual data set that we use to train the model (weights and biases in the case of Neural Network). The model sees and learns from this data.

**Validation Dataset**
Validation Data set: The sample of data used to provide an unbiased evaluation of a model fit on the training data set while tuning model hyper parameters. The evaluation becomes more biased as skill on the validation data set is incorporated into the model configuration.
The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model's hyper parameters. Hence the model occasionally sees this data, but never does it "Learn" from this. 

**Test Dataset**
Test Data set: The sample of data used to provide an unbiased evaluation of a final model fit on the training data set.
The Test data set provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets).

\pagebreak

### Classification using Nearest Neighbor

Let's start with the classification task.  Given a set of labeled points, we want to be able to classify some unlabeled points. The knn algorithm tries to classify a new record or entity by locating the new record's *k* nearest neighbors. 


 ![](knn.png){width=350px}
 
R has a host of distance formulas with the dist() command, but in this case we will look at the Euclidean distance - or straight line distance between the two points.  The algorithm looks at the new entries *k* nearest neighbors and determines which group the new entry falls into.  Lets look at our example.  The size of *k* is rather important and there is a potential for too much variance or over fitting based on the number of required neighbors.

### Rescaling Data ###
Notice that in our example, SAT scores Range from 600-1600 and APFT scores Range from 0-300.  This difference in scale could result in groupings weighted unfairly.  Therefore it is important to re-scale the data. Two popular ways are Normalization and Z- Score Standardization. The same re-scaling method must be applied across all variables.

$$X_{new}=\frac{X-X_{min}}{X_{max}-X_{min}}$$
Using the z-score standardization (R function `scale`).  $$X_{new}=\frac{X-\mu}{\sigma}$$

Categorical variables can all just be classified as 1 or 0 dummy variables (known as One-Hot Encoding).

## Classifying hand writing examples

### Step 1 - Collect and clean the data

We will utilize optdigits data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/).

This data set has 5620 records with 65 fields. The 65th field is the actual digit (0-9) and the other 64 fields describe how many pixels are used in a particular grid. The idea being that no matter who writes a particular number, all instances of 1, 6, whichever number should use roughly the same pixels. 

```{r, include=F}
opt <- read_csv('optdigits.txt')
```


```{r, include=F}
opt.showme = function (data, i, ...)
{
  z <- unlist (data[i, 1:64])
  names (z) <- NULL
  z <- matrix (z, 8, 8)
  z <- z[,8:1]
  image (z = z, col = gray ((15:0)/15), main = paste ("This is a", data[i, 65]))
}
```


```{r, echo=F}
# show what the data represent
par(mfrow=c(2,2))
opt.showme(opt, 5);opt.showme(opt, 34)
opt.showme(opt, 55);opt.showme(opt, 59)
```

### Step 2 - Explore and prepare the data

When dealing with categorical targets, most machine learning algorithms prefer the target feature to be a type FACTOR. We'll also rename the 65^th column as "Target" for clarity.

```{r}
opt %<>% # magrittr assign/pipe combo
  rename("Target" = "X65") %>% 
  mutate(Target = factor(Target))
```

Tables are usually good ways to explore categorical targets. We want to make sure our response values are generally balanced. Not a significant issue for the purposes of the algorithm if they are not, but it is good to know.

\pagebreak
```{r}
opt %>% 
  group_by(Target) %>% 
  summarize(n=n(), prop=n()/nrow(opt))
```

The response looks pretty balanced, each digit making up about 10% of the total data.

#### Normalize the data ####

Take a look at the data, do we need to normalize or scale it in any way?
```{r, include=FALSE}
ggplot(opt %>% 
         select(-Target) %>% 
         pivot_longer(
           everything(),
           names_to='obs',
           values_to='val'))+
  geom_boxplot(aes(x=obs, y=val))
```
Some pixels appear more used than others, but it looks like the data are already scaled 0-16. Also, the data description tells us as much but always a good idea to check. 

However, if you did need to scale the data, here is a function to normalize the data between the values of 0 to 1. Or if you want Z-scores use the `scale` function.

```{r}
normalize<-function(x) {
    return((x-min(x))/(max(x)-min(x)))
}
normalize(c(1,2,3,4,5))
```

Now we want to split our data into a training set and a validating set. For this, lets assume our test data is sealed in a vault somewhere and the remaining data is ours to use. A common rule of thumb for starting off is 80% training and 20% validating.  The current data set is already randomly sorted, but for sake of practice we will randomly sort the data and separate the data set (set a seed for reproducibility).

This is a common way of doing the split in R and the process is similar in other tools (it might be automated). Here we will use the sample command to randomly generate a vector of integers that include the numbers 1 through $n$. We will use those index values to slice data. Finally we remove `Target` from our predictive data set when using knn.

```{r}
set.seed(45)
i <- sample(1:nrow(opt), floor(0.8*nrow(opt)))
optTrain <- # create the training set
  opt %>% 
  slice(i) %>% # keep the values in the ith position
  select(-Target) # remove Target

optTrainTarget <- # create the training target vector
  opt %>% 
  slice(i) %>% # keep the values in the ith position
  pull(Target) # keep only the target values

optVal <- # create the validation set
  opt %>% 
  slice(-i) %>% # remove the ith values
  select(-Target) # remove Target

optValTarget <- # create the validation target vector
  opt %>% 
  slice(-i) %>% # remove the ith values
  pull(Target) # keep only the target values
```

### Step 3 - Train the model

knn training requires no model building, it is just grouping the data.  For this example we can use the `knn` function from the `class` package. The function returns a factor vector of predicted classes for each row in the validation data set. The `knn` function requires the training data, the validation data, and the number of classes (or neighbors) we want to use. Part of the process could be to test different class amounts and assess differences in performance.

```{r}
library(class)
#use knn given the training data, predict the validate date regarding the classes in trainLabels
optKNN <- knn(optTrain, optVal, optTrainTarget, 3)
```

### Step 4 - Evaluate the model

The easiest way to evaluate the model is to use it on the validation set and see how many we got right. A table works great for this. The diagonals are the correct selection. There are many other methods from other packages out there, choose the one you like the best.

This type of table is called a confusion matrix and it provides a wealth of information and can be misleading. The values on the matrix diagonal are the correct predictions. We use correct predictions to determine the model accuracy.

* Accuracy: $\frac{Correct Predictions}{All Predictions}$

Although accuracy is an important metric, it is not always adequate to measure a model's performance and can be misleading when there is class imbalance.  An example of class imbalance: Person is in the NBA, Person is not in the NBA. If the model just always selects NOT, it can be correct 99.9% of the time - even if we get every NBA player wrong.  We have an accurate model, but it is worthless.

* Positive Predictive Value also called Precision $\frac{True Positive}{True Positive+False Positive}$

From all the actual instances of a class how often did the model predict said class.

* Sensitivity or True Positive Rate: $\frac{True Positive}{All Positives}$ 

So in words - out of the total number of 5s in our data set, how many did we get correct?

I do not show this to make you experts, but to expose - from the text "An Introduction to Statistical Learning" ..."There is an almost bewildering array of terms used in this context".

\pagebreak
```{r}
(tab <- table(optKNN, optValTarget))
accuracy<-sum(diag(tab))/sum(tab) #Number Correct/Total Number
accuracy
```

Change the number of nearest neighbors to determine the best model.  

This is a fun way to show the same info graphically.

```{r,fig.height = 3, fig.width = 4, fig.align = "center"}
library(ggplot2)
newData<-data.frame(Real=factor(optValTarget),
                    Predict = factor(optKNN))
# newData %<>%
#   mutate(
#     optValTarget = factor(optValTarget), 
#     optKNN = factor(optKNN))
ggplot(
  newData,
  aes(optKNN,optValTarget,color=optValTarget))+
  geom_jitter(width=.3,height=.3,size=1)
```

Or we can use the command `confusionMatrix` from the Caret Package which comes with more info.

```{r,eval=FALSE}
library(caret)
confusionMatrix(optKNN, optValTarget)
```

### Exercise  
The Human Activities Recognition data set that we are using is a collection of accelerometer data taken from a smartphone that various people carried with them while conducting six different exercises (Downstairs, Jogging, Sitting, Standing, Upstairs, Walking). For each exercise the acceleration for the x, y, and z axis was measured and captured with a time-stamp and person ID. The Target Variable is which activity the person was conducting. Use the KNN algorithm to build a model to classify the exercise.  Then assess the model. Change the number of nearest neighbors to determine the best model.

```{r,eval=FALSE}
library(ggplot2)
har<-read_csv('clean_har.csv',na = "")
har_n<-as.data.frame(lapply(har[,-52],normalize))

har_n <- 
  har %>% 
  select(-classe) %>% 
  mutate(across(everything(), normalize))

i <- sample(1:nrow(har_n), size = floor(.8*nrow(har_n)))

harTrain <- har_n %>% slice(i)
harVal <- har_n %>% slice(-i)
trainLabels <- har %>% slice(i) %>% pull(classe)
valLabels <- har %>% slice(-i) %>% pull(classe)

harPred <- knn(harTrain,harVal,trainLabels,5)
(harTable <- table(harPred,valLabels))
(acc <- sum(diag(harTable))/sum(harTable))
newData <- data.frame(Predicted = harPred, Real = valLabels)
ggplot(newData,
       aes(Predicted,Real,color=Real))+
  geom_jitter(width=.3,height=.3,size=1)

```